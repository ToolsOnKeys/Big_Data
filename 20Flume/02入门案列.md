# Flume入门案例



## 1、监控端口数据

### ①、案例需求

* 使用Flume监听一个端口，收集该端口数据，并打印到控制台

### ②、需求分析

![](D:\BigData\BigData\20Flume\相关图片\监听数据端口案列分析.png)

### ③、实现步骤（文件配置）

``` linux
#1、安装netcat工具
> yum install -y nc
#2、判断nc端口是否被占用
> sudo netstat -tunlp | grep 44444
#3、创建Flume Agent配置文件 f1.conf
> pwd     # /opt/module/flume/homework/confs
> vim f1.conf

# 命名source、sinks和channels
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 设定source
## 设定sources的输入员类型为netcat
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 44444

# 设定sinks
a1.sinks.k1.type = logger

# 设定channels
a1.channels.c1.type = memory
## 总容量为10000个event
a1.channels.c1.capacity = 10000
## 表示传输时收集到10000个event以后才会去提交事务
a1.channels.c1.transactionCapacity = 10000
a1.channels.c1.byteCapacityBufferPercentage = 20
a1.channels.c1.byteCapacity = 800000

# 设定source、sinks和channels之间的连接关系
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

### ④、开启flume监听端口

```linux
# 方式一
> bin/flume-ng agent --conf conf/ --name a1 --conf-file homework/confs/f1.conf -Dflume.root.logger=INFO,console
# 方式二
> bin/flume-ng agent -c conf/ -n a1 -f homework/confs/f1.conf -Dflume.root.logger=INFO,console
```

### ⑤、使用netcat工具先本机指定端口发送内容

```linux
> nc localhost 44444
```

### ⑥、在Flume监听页面观察接收地数据情况

## 2、实时监控单个追加文件

### ①、案例需求

* 实时监控Hive日志，并上传到HDFS中

### ②、需求分析

![](D:\BigData\BigData\20Flume\相关图片\实时读取本地文件到HDFS案例分析.png)

### ③、实现步骤（文件配置）

* Flume 要想将数据输出到HDFS，须持有Hadoop相关Hadoop的相关jar包

  ```jar
  commons-configuration-1.6.jar
  hadoop-auth-2.7.2.jar
  hadoop-common-2.7.2.jar
  hadoop-hdfs-2.7.2.jar
  commons-io-2.4.jar
  htrace-core-3.1.0-incubating.jar
  ```

* 创建f2.conf配置文件

  ``` conf
  # 配置source、sinks和channels
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1
  
  #配置source，监控本地hive的日志文件
  a1.sources.r1.type = exec
  a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log
  a1.sources.r1.channels = c1
  
  #配置sinks
  a1.sinks.k1.type = hdfs
  a1.sinks.k1.hdfs.path = hdfs://hadoop101:9000/flumehomework/hivelogs/%y-%m-%d/%H%M/%S
  #上传文件的前缀
  a1.sinks.k1.hdfs.filePrefix = logs-
  #是否按照时间滚动文件夹
  a1.sinks.k1.hdfs.round = true
  #多少时间单位创建一个新的文件夹
  a1.sinks.k1.hdfs.roundValue = 20
  #重新定义时间单位
  a1.sinks.k1.hdfs.roundUnit = sec
  #是否使用本地时间戳
  a1.sinks.k1.hdfs.useLocalTimeStamp = true
  #积攒多少个Event才flush到HDFS一次
  a1.sinks.k1.hdfs.batchSize = 1000
  #设置文件类型，可支持压缩
  a1.sinks.k1.hdfs.fileType = DataStream
  #多久生成一个新的文件
  a1.sinks.k1.hdfs.rollInterval = 30
  #设置每个文件的滚动大小
  a1.sinks.k1.hdfs.rollSize = 134217700
  #文件的滚动与Event数量无关
  a1.sinks.k1.hdfs.rollCount = 0
  
  # 设定channels
  a1.channels.c1.type = memory
  ## 总容量为10000个event
  a1.channels.c1.capacity = 10000
  ## 表示传输时收集到10000个event以后才会去提交事务
  a1.channels.c1.transactionCapacity = 10000
  a1.channels.c1.byteCapacityBufferPercentage = 20
  a1.channels.c1.byteCapacity = 800000
  
  # 设定source、sinks和channels之间的连接关系
  a1.sources.r1.channels = c1
  a1.sinks.k1.channel = c1
  ```

### ④、开启flume监听端口

```linux
> bin/flume-ng agent -c conf/ -n a1 -f homework/confs/f2.conf
```

### ⑤、开启Hadoop和Hive并操作Hive产生日志

### ⑥、在HDFS上查看文件

## 3、实时监控目录下多个新文件

### ①、案例需求

* 使用flume监听整个目录下的文件，并上传至HDFS。
* 在Spooling Directory Source时，不要在监控目录中创建并持续修改文件
* 上传完成的文件会议.COMPLETED结尾
* 被监控的文件夹每500毫秒扫描一次文件变动

### ②、需求分析

![](D:\BigData\BigData\20Flume\相关图片\实时读取目录文件到hdfs案例.png)

### ③、实现步骤

* 配置文件f3.conf

  ``` conf
  # 配置source、sinks和channels
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1
  
  
  #配置source，监控本地hive的日志文件
  a1.sources.r1.type = spooldir
  a1.sources.r1.spoolDir = /opt/module/flume/homework/datas
  a1.sources.r1.fileHeader = true
  a1.sources.r1.fileSuffix = .COMPLETED
  a1.sources.r1.fileHeader = true
  #忽略所有以.tmp结尾的文件，不上传
  a1.sources.r1.ignorePattern = ([^ ]*\.tmp)
  
  #配置sinks
  a1.sinks.k1.type = hdfs
  a1.sinks.k1.hdfs.path = hdfs://hadoop101:9000/flumehomework/filelog/%y-%m-%d/%H
  #上传文件的前缀
  a1.sinks.k1.hdfs.filePrefix = file-
  #是否按照时间滚动文件夹
  a1.sinks.k1.hdfs.round = true
  #多少时间单位创建一个新的文件夹
  a1.sinks.k1.hdfs.roundValue = 20
  #重新定义时间单位
  a1.sinks.k1.hdfs.roundUnit = sec
  #是否使用本地时间戳
  a1.sinks.k1.hdfs.useLocalTimeStamp = true
  #积攒多少个Event才flush到HDFS一次
  a1.sinks.k1.hdfs.batchSize = 1000
  #设置文件类型，可支持压缩
  a1.sinks.k1.hdfs.fileType = DataStream
  #多久生成一个新的文件
  a1.sinks.k1.hdfs.rollInterval = 30
  #设置每个文件的滚动大小
  a1.sinks.k1.hdfs.rollSize = 134217700
  #文件的滚动与Event数量无关
  a1.sinks.k1.hdfs.rollCount = 0
  
  # 设定channels
  a1.channels.c1.type = memory
  ## 总容量为10000个event
  a1.channels.c1.capacity = 10000
  ## 表示传输时收集到10000个event以后才会去提交事务
  a1.channels.c1.transactionCapacity = 10000
  a1.channels.c1.byteCapacityBufferPercentage = 20
  a1.channels.c1.byteCapacity = 800000
  
  # 设定source、sinks和channels之间的连接关系
  a1.sources.r1.channels = c1
  a1.sinks.k1.channel = c1
  ```

### ④、启动监控命令

```linux
> bin/flume-ng agent -c conf/ -n a1 -f homework/confs/f3.conf
```

### ⑤、向指定文件夹中添加新文件

### ⑥、到hdfs上查看数据

## 4、实时监控目录下多个追加文件

* Exec source 适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir Source能够保证数据不丢失，且能够实现断电续传，但是延迟较高，不能实现实时监控；而Taildir Source既能够实现断电续传，又可以保证数据不丢失，还能够进行实时监控。
* Taildir 说明：Taildir Source维护了一个json格式的position File，其会定期的玩position File中更新每个文件读取到的最新的位置，因此能够实现断电续传。
* Linux中存储文件元数据的区域就叫做inode，每个inode都有一个号码，操作系统用inode号码来识别不同的文件，Unix/Linux系统内部不是文件名，而是用inode号码来识别文件。

### ①、案例需求

* 使用Flume监听整个目录的实时追加文件，并上传至HDFS

### ②、需求分析

![](D:\BigData\BigData\20Flume\相关图片\实时读取目录文件到HDFS案例 (2).png)

### ③、实现步骤

* 创建配置文件f4.conf

  ```conf
  # 配置source、sinks和channels
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1
  
  
  #配置source，监控本地hive的日志文件
  a1.sources.r1.type = TAILDIR
  a1.sources.r1.positionFile = /opt/module/flume/homework/datas/taidir.json
  a1.sources.r1.filegroups = f1
  a1.sources.r1.filegroups.f1 =/opt/module/flume/homework/datas/f4datas/*
  
  #配置sinks
  a1.sinks.k1.type = hdfs
  a1.sinks.k1.hdfs.path = hdfs://hadoop101:9000/flumehomework/f4-filelog/%y-%m-%d/%H
  #上传文件的前缀
  a1.sinks.k1.hdfs.filePrefix = file-
  #是否按照时间滚动文件夹
  a1.sinks.k1.hdfs.round = true
  #多少时间单位创建一个新的文件夹
  a1.sinks.k1.hdfs.roundValue = 20
  #重新定义时间单位
  a1.sinks.k1.hdfs.roundUnit = sec
  #是否使用本地时间戳
  a1.sinks.k1.hdfs.useLocalTimeStamp = true
  #积攒多少个Event才flush到HDFS一次
  a1.sinks.k1.hdfs.batchSize = 1000
  #设置文件类型，可支持压缩
  a1.sinks.k1.hdfs.fileType = DataStream
  #多久生成一个新的文件
  a1.sinks.k1.hdfs.rollInterval = 30
  #设置每个文件的滚动大小
  a1.sinks.k1.hdfs.rollSize = 134217700
  #文件的滚动与Event数量无关
  a1.sinks.k1.hdfs.rollCount = 0
  
  # 设定channels
  a1.channels.c1.type = memory
  ## 总容量为10000个event
  a1.channels.c1.capacity = 10000
  ## 表示传输时收集到10000个event以后才会去提交事务
  a1.channels.c1.transactionCapacity = 10000
  a1.channels.c1.byteCapacityBufferPercentage = 20
  a1.channels.c1.byteCapacity = 800000
  
  # 设定source、sinks和channels之间的连接关系
  a1.sources.r1.channels = c1
  a1.sinks.k1.channel = c1
  ```

### ④、启动监控文件夹命令

```linux
> bin/flume-ng agent -c conf/ -n a1 -f homework/confs/f4.conf
```

### ⑤、向指定监控文件夹中添加文件

### ⑥、到hdfs上查看数据